---
layout: topicmodeling
title: TopicModeling
banner: "/assets/banners/topic4.png"
heading: 'Topic Modeling'
subheading: 'Which topics do we lisen to the most?'
---

<h2>Topic Modelling goal</h2>
<p>
The topic modelling part of this study aim to investigate what lyrics we actually listen to and fills our minds with when we play our favourite songs. 
  We want to research if some topics are more popular than others and if the topics depend on the genre. 
Our study begins by looking at the most frequence words and see if some patterns can be drawn. Frome there we continue investigating the topics by
  cleaning the text and reducing the vocabulary size. The topic modelling is done on the cleaned data to reveal topics within the songs. 
  In order to get an even deeper insight into the music industri we investigate each genre at a time to see if they have their own individual topics. 
  The goal is to understand the words we listen to and to discover if the topics and messages from the lyrics depends on our choice of genre.
  
</p>

<h2>Topic Modelling tool</h2>

<p>
 The study will use Latent DirichletAllocation (LDA) to perform topic modeling on the data.  
  The LDA model consists of two main procedures; generating topics and assigning topics to each text. 
  Each text will then be described by a distribution of topics and each topic can in turn be described by a distribution of words.
  
  A necessary step for all topic models is to reduce corpus dimensionality, i.e. the total number of unique words, as much as possible prior to model implementation.
  The main reason being that unnecessary words adds too much noise when finding meaningful patterns. 
  
  
</p>   

<h2>Frequency count befor cleaning</h2>
<p>
As a priliminary investigation we look at the most frequence words within all songs to investigate if some topics is revealed. <br> </p>

<p> Top 10 most common words in the dataset:<br></p>


<table>
  <colgroup>
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
</colgroup>
  <tr>
     <th>Word</th>
    <td>I</td>
    <td>the</td> 
    <td>you</td>
    <td>a</td>
    <td>to</td>
        <td>my</td>
    <td>me</td>
    <td>and</td>
        <td>in</td>
    <td>it</td>
  </tr>
  <tr>
    <th>Count</th> 
    <td>3813</td>
    <td>3420</td> 
    <td>3015</td>
        <td>2082</td> 
    <td>1950</td>
        <td>1707</td>
        <td>1512</td> 
    <td>1300</td>
            <td>1234</td> 
    <td>1216</td>
  </tr>
</table>



It is clear that the frequenced words are stopwords without meaning. 

   A thorough text cleaning is needed  in order to ensure a qualified topic modeling performance.
.</p>

<h2>Text cleaning</h2>

<p>

  
  
The  aim  of  text  cleaning  is  to  significantly  reduce  corpus  dimensionality. 
  The  quality  of  the definitive topics is highly dependent on the quality of the input data for the model. 
  
  The lyrics will undergo four different cleaning steps in order to produce the input for the topic model.  
  Below an overview of the full cleaning process canbe seen and the resulting vocabulary size after having applied the different cleaning techniques.
</p>

<p style="text-align:center;">
  <img src="voc.png" width="700" hight="500" >                                      
</p>

<p>
Starting with the raw corpus of 12,647 unique words reduced to 2,810 words in the ultimate output yields a total word reduction of 78%.  <br>
The four procedural steps are to be explained:
</p>

<p> &nbsp;&nbsp; 1. Noice</p>
<p> 
First, a preliminary noise cleaning of the text data is carried out, in which all words are transformed into lowercase capitalization, 
  ordinary stop-words such as ”the”, ”have” and ”should” are removed along with non-alphabetic characters (!?,.). 
  It is followed by lemmatization to extract the dictionary form of all words. 


<p> &nbsp;&nbsp; 2. Rare and short words</p>
<p>
A simple but efficient text cleaning step in terms of vocabulary reduction, is to remove rare and short words. 
  All words that are only mentioned one time in the entire dataset of lyrics are deemed too unique and without importance, hence they are deleted. 
  The same applies to words of only one character. 
</p>

<p> &nbsp;&nbsp; 3. Only noun, verb and adj</p>
<p>
 All words that are not labeled ”noun”, ”verb” or "adj" are removed as it is expected that the nouns, verbs and adjectives carry most 
  information on the message of a song.
</p>

<p> &nbsp;&nbsp; 4. Meaningless words</p>
<p>
As the larst cleaning step we looked at the frequency words and removed words with no particulary meaning for the overall messages. This will help the model perform. 
</p>

<p>
The text cleaning of the lyrics is now finalized and unimportant words have been removed. The cleaned data is not 
 ready to be used as input for a topic model.  

</p>

<h2>Frequency count after cleaning</h2>

<p>
It is of interest to examine the most frequent terms after the cleaning to detecting if possible topics appears now. 
</p>

<p> Top 10 most common words after cleaning<br></p>

<table>
    <colgroup>
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
</colgroup>
  <tr>
     <th>Word</th>
    <td>love</td>
    <td>nigga</td> 
    <td>baby</td>
    <td>bitch</td>
    <td>fuck</td>
        <td>life</td>
    <td>shit</td>
    <td>money</td>
        <td>good</td>
    <td>play</td>
  </tr>
  <tr>
    <th>Count</th> 
    <td>569</td>
    <td>400</td> 
    <td>327</td>
        <td>268</td> 
    <td>207</td>
        <td>203</td>
        <td>188</td> 
    <td>171</td>
            <td>166</td> 
    <td>159</td>
  </tr>
</table>

<p>
 As hoped the top frequency words for the cleaned data indicates and reveals some possible topics, both some romance, wealth and hate.
  </p>

<h2>Topics</h2>

<p>
   The topic model is implemented to generate topics and label each song with the most appropriate topic.
  Based on a measure of coherence,the model determines the most optimal number of topics to represent the data. <br></p>
  
  <p style="text-align:center;">
  <img src="cor.png" width="700" hight="500" >                                      
</p>
  
  <p>
  As per the coherence measure,n=6 topics yield the most optimal representation. 
   The LDA generated topics cn be seen below including relevant keywords and their respective average sentiment score. </p>

<table>
  <colgroup>
<col width="30%" />
<col width="30%" />
<col width="20%" />
<col width="20%" />
</colgroup>
  <tr>
     <th>Topic</th>
<th>Keywords</th>
    <th>Avg. sentiment score</th>
    <th>Number of songs</th>
  </tr>
  <tr>
    <td>(1) Violent gangster</td>
    <td>money, nigga, shit, hit</td> 
    <td>0.22</td>
        <td>46</td>
  </tr>
    <tr>
    <td>(2) Ups and downs in romance</td>
    <td>love, heart, pain, lose, try</td> 
    <td>0.21</td>
        <td>52</td>
  </tr>
    <tr>
    <td>(3) Breaking rules</td>
    <td>bitch, cheat, play, fuck</td> 
    <td>0.10</td>
        <td>62</td>
  </tr>
    <tr>
    <td>(4) Life experience</td>
    <td>world, mine, fine, dream, die</td> 
    <td>0.58</td>
        <td>101</td>
  
</table>

<p>
  The table above shows that topic 3 is the most negative topic and topic 4 is the most positive. This does allign with the keywords for each of the topics. 
  Furthermore it can be seen that the largest topics within songs is topic 4. 
  That the most positive topic also is the largest fits with the fact that the sentiment analysis showed most positive songs. 
</p>
  
  <p>
  Below you see an interactive LDA visualizer, which has been used to examine generated topics. The chart to the left illustrate how separated the classes are
    whereas the chart to the right shows the words that reprsents the topics ordered by relevance depending on the tunable lambda-parameter. 
    The λ-parameter represents the TF-IDF adjustment, which assigns less importance to words with high frequency count,
    λ=1 corresponds to importance by term frequency. Decreasing the parameter yields less importance to more frequently used words.
</p>


{% include lda.html %}

<p>
  dldldl...
</p>

<h2>Topics pr genre</h2>

<p>
 LDA topic modelling is implemented on all genres in order to discover topics within each genre. We are interested in investgating if different genres has different topics in their lyrics.  
</p>

<h2>Wordclouds pr genre</h2>
<p>
The wordclouds below illustrates words within each topic. The size of the words represents their relative relevance.
</p>
<p style="text-align:center;">
  <img src="genreWC.png" width="1000" height="360">                                      
</p>

                                           
<h2>Topic Modelling conclusion</h2>

<p>
h
</p>   
